# Module 4 Summary - Analytics Engineering with dbt

#DataEngineeringZoomcamp #dbt #AnalyticsEngineering #DataModeling

---

## Part 1: Introduction to Analytics Engineering & dbt Fundamentals ğŸ¯

### What is Analytics Engineering?

### The Evolution of Data Roles

Traditionally, there were two main roles in data:

| Role | Focus | Skills |
|------|-------|--------|
| **Data Engineer** | Building pipelines, infrastructure, data movement | Python, Spark, Airflow, cloud services |
| **Data Analyst** | Creating reports, dashboards, insights | SQL, Excel, BI tools |

But there was a gap! Who transforms the raw data into clean, analysis-ready tables? Enter the **Analytics Engineer**.

### What Does an Analytics Engineer Do?

An Analytics Engineer sits between Data Engineering and Data Analytics:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Engineer  â”‚ â”€â”€â–º â”‚  Analytics Engineer  â”‚ â”€â”€â–º â”‚   Data Analyst  â”‚
â”‚                 â”‚     â”‚                      â”‚     â”‚                 â”‚
â”‚  â€¢ Pipelines    â”‚     â”‚  â€¢ Transform data    â”‚     â”‚  â€¢ Dashboards   â”‚
â”‚  â€¢ Infrastructureâ”‚    â”‚  â€¢ Data modeling     â”‚     â”‚  â€¢ Reports      â”‚
â”‚  â€¢ Data movementâ”‚     â”‚  â€¢ Quality tests     â”‚     â”‚  â€¢ Insights     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â€¢ Documentation     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key responsibilities:**
- ğŸ“Š Transform raw data into clean, modeled datasets
- ğŸ§ª Write tests to ensure data quality
- ğŸ“ Document everything so others can understand
- ğŸ”— Build the "T" in ELT (Extract, Load, Transform)

### The Kitchen Analogy ğŸ³

Think of a data warehouse like a restaurant:

| Restaurant | Data Warehouse | Who accesses it |
|------------|----------------|-----------------|
| **Pantry** (raw ingredients) | Staging area (raw data) | Data Engineers |
| **Kitchen** (cooking happens) | Processing area (transformations) | Analytics Engineers |
| **Dining Hall** (served dishes) | Presentation area (final tables) | Business users, Analysts |

Raw ingredients (data) come in, get processed (transformed), and are served as polished dishes (analytics-ready tables).

---

## What is dbt? ğŸ› ï¸

**dbt** stands for **data build tool**. It's the most popular tool for analytics engineering.

### The Problems dbt Solves

Before dbt, data transformation was messy:
- âŒ SQL scripts scattered everywhere with no organization
- âŒ No version control (changes got lost)
- âŒ No testing (errors discovered too late)
- âŒ No documentation (nobody knew what anything meant)
- âŒ No environments (changes went straight to production!)

**dbt brings software engineering best practices to analytics:**
- âœ… **Version control** - Your SQL lives in Git
- âœ… **Modularity** - Reusable pieces instead of copy-paste
- âœ… **Testing** - Automated data quality checks
- âœ… **Documentation** - Generated from your code
- âœ… **Environments** - Separate dev and prod

### How dbt Works

dbt follows a simple principle: **write SQL, dbt handles the rest**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Your dbt Project                        â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚  models/*.sql â”‚â”€â”€â”€â–ºâ”‚   dbt compile â”‚â”€â”€â”€â–ºâ”‚ SQL Queriesâ”‚ â”‚
â”‚   â”‚  (your logic) â”‚    â”‚   dbt run     â”‚    â”‚ (executed) â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                              â”‚
â”‚                              â–¼                              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                    â”‚  Data Warehouse  â”‚                     â”‚
â”‚                    â”‚  (views/tables)  â”‚                     â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. You write SQL files (called "models")
2. dbt compiles them (adds warehouse-specific syntax)
3. dbt runs them against your data warehouse
4. Views/tables are created automatically!

### dbt Core vs dbt Cloud

| Feature | dbt Core | dbt Cloud |
|---------|----------|-----------|
| **Cost** | Free (open source) | Free tier + paid plans |
| **Where it runs** | Your machine/server | Cloud-hosted |
| **Setup** | Manual installation | Browser-based IDE |
| **Scheduling** | Need external tool | Built-in scheduler |
| **Best for** | Local development, cost savings | Teams, ease of use |

ğŸ’¡ **For this course:** You can use either! Local setup uses DuckDB + dbt Core (free). Cloud setup uses BigQuery + dbt Cloud.

---

## Part 2: dbt Project Structure & Building Models ğŸ“

### Why Model Data? ğŸ“

Raw data is messy and hard to query. Dimensional modeling organizes data into a structure that's:
- Easy to understand
- Fast to query
- Flexible for different analyses

### Fact Tables vs Dimension Tables

This is the core of dimensional modeling (also called "star schema"):

**Fact Tables (`fct_`)**
- Contain **measurements** or **events**
- One row per thing that happened
- Usually have many rows (millions/billions)
- Contain numeric values you want to analyze

**Examples:**
- `fct_trips` - one row per taxi trip
- `fct_sales` - one row per sale
- `fct_orders` - one row per order

```sql
-- Example fact table
CREATE TABLE fct_trips AS
SELECT
    trip_id,           -- unique identifier
    pickup_datetime,   -- when it happened
    dropoff_datetime,
    pickup_zone_id,    -- foreign keys to dimensions
    dropoff_zone_id,
    fare_amount,       -- numeric measures
    tip_amount,
    total_amount
FROM transformed_trips;
```

**Dimension Tables (`dim_`)**
- Contain **attributes** or **descriptive information**
- One row per entity
- Usually fewer rows
- Provide context for fact tables

**Examples:**
- `dim_zones` - one row per taxi zone
- `dim_customers` - one row per customer
- `dim_products` - one row per product

```sql
-- Example dimension table
CREATE TABLE dim_zones AS
SELECT
    location_id,       -- primary key
    borough,           -- descriptive attributes
    zone_name,
    service_zone
FROM zone_lookup;
```

### The Star Schema â­

When you join facts and dimensions, you get a star shape:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  dim_zones   â”‚
                    â”‚  (pickup)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dim_vendors â”‚â”€â”€â”€â”€â”‚  fct_trips   â”‚â”€â”€â”€â”€â”‚  dim_zones   â”‚
â”‚              â”‚    â”‚  (center)    â”‚    â”‚  (dropoff)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â”‚ dim_payment  â”‚
                    â”‚    types     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why it's powerful:**
```sql
-- Easy to answer business questions!
SELECT 
    z.borough,
    COUNT(*) as trip_count,
    SUM(f.total_amount) as total_revenue
FROM fct_trips f
JOIN dim_zones z ON f.pickup_zone_id = z.location_id
GROUP BY z.borough
ORDER BY total_revenue DESC;
```

---

### dbt Project Structure

A dbt project has a specific folder structure. Understanding this helps you navigate any project:

```
taxi_rides_ny/
â”œâ”€â”€ dbt_project.yml      # Project configuration (most important!)
â”œâ”€â”€ profiles.yml         # Database connection (often in ~/.dbt/)
â”œâ”€â”€ packages.yml         # External packages to install
â”‚
â”œâ”€â”€ models/              # â­ YOUR SQL MODELS LIVE HERE
â”‚   â”œâ”€â”€ staging/         # Raw data, minimally cleaned
â”‚   â”œâ”€â”€ intermediate/    # Complex transformations
â”‚   â””â”€â”€ marts/           # Final, business-ready tables
â”‚
â”œâ”€â”€ seeds/               # CSV files to load as tables
â”œâ”€â”€ macros/              # Reusable SQL functions
â”œâ”€â”€ tests/               # Custom test files
â”œâ”€â”€ snapshots/           # Track data changes over time
â””â”€â”€ analysis/            # Ad-hoc queries (not built)
```

### The `dbt_project.yml` File

This is the **most important file** - dbt looks for it first:

```yaml
name: 'taxi_rides_ny'
version: '1.0.0'
profile: 'taxi_rides_ny'  # Must match profiles.yml!

# Default configurations
models:
  taxi_rides_ny:
    staging:
      materialized: view  # Staging models become views
    marts:
      materialized: table # Mart models become tables
```

### The Three Model Layers

dbt recommends organizing models into three layers:

**1. Staging Layer (`staging/`)**

**Purpose:** Clean copy of raw data with minimal transformations

**What happens here:**
- Rename columns (snake_case, clear names)
- Cast data types
- Filter obviously bad data
- Keep 1:1 with source (same rows, similar columns)

```sql
-- models/staging/stg_green_tripdata.sql
{{ config(materialized='view') }}

with tripdata as (
    select * 
    from {{ source('staging', 'green_tripdata') }}
    where vendorid is not null  -- filter bad data
)

select
    -- Rename and cast columns
    cast(vendorid as integer) as vendor_id,
    cast(lpep_pickup_datetime as timestamp) as pickup_datetime,
    cast(lpep_dropoff_datetime as timestamp) as dropoff_datetime,
    cast(pulocationid as integer) as pickup_location_id,
    cast(dolocationid as integer) as dropoff_location_id,
    cast(passenger_count as integer) as passenger_count,
    cast(trip_distance as numeric) as trip_distance,
    cast(fare_amount as numeric) as fare_amount,
    cast(total_amount as numeric) as total_amount
from tripdata
```

**2. Intermediate Layer (`intermediate/`)**

**Purpose:** Complex transformations, joins, business logic

**What happens here:**
- Combine multiple staging models
- Apply business rules
- Heavy data manipulation
- NOT exposed to end users

```sql
-- models/intermediate/int_trips_unioned.sql
with green_trips as (
    select *, 'Green' as service_type
    from {{ ref('stg_green_tripdata') }}
),

yellow_trips as (
    select *, 'Yellow' as service_type
    from {{ ref('stg_yellow_tripdata') }}
)

select * from green_trips
union all
select * from yellow_trips
```

**3. Marts Layer (`marts/`)**

**Purpose:** Final, business-ready tables for end users

**What happens here:**
- Final fact and dimension tables
- Ready for dashboards and reports
- Only these should be exposed to BI tools!

```sql
-- models/marts/fct_trips.sql
{{ config(materialized='table') }}

select
    t.trip_id,
    t.service_type,
    t.pickup_datetime,
    t.dropoff_datetime,
    t.pickup_location_id,
    t.dropoff_location_id,
    z_pickup.zone as pickup_zone,
    z_dropoff.zone as dropoff_zone,
    t.passenger_count,
    t.trip_distance,
    t.fare_amount,
    t.total_amount
from {{ ref('int_trips_unioned') }} t
left join {{ ref('dim_zones') }} z_pickup 
    on t.pickup_location_id = z_pickup.location_id
left join {{ ref('dim_zones') }} z_dropoff 
    on t.dropoff_location_id = z_dropoff.location_id
```

---

### Sources and the `source()` Function ğŸ“¥

### What are Sources?

Sources tell dbt where your raw data lives in the warehouse. They're defined in YAML files:

```yaml
# models/staging/sources.yml
version: 2

sources:
  - name: staging           # Logical name (you choose)
    database: my_project    # Your GCP project or database
    schema: nytaxi          # BigQuery dataset or schema
    tables:
      - name: green_tripdata
      - name: yellow_tripdata
```

### Using the `source()` Function

Instead of hardcoding table names, use `source()`:

```sql
-- âŒ Bad - hardcoded path
SELECT * FROM my_project.nytaxi.green_tripdata

-- âœ… Good - using source()
SELECT * FROM {{ source('staging', 'green_tripdata') }}
```

**Benefits:**
- Change database/schema in one place (YAML file)
- dbt tracks dependencies automatically
- Can add freshness tests on sources

---

### The `ref()` Function - Building Dependencies ğŸ”—

This is **the most important dbt function!**

### `source()` vs `ref()`

| Function | Use When | Example |
|----------|----------|---------|
| `source()` | Reading raw/external data | `{{ source('staging', 'green_tripdata') }}` |
| `ref()` | Reading another dbt model | `{{ ref('stg_green_tripdata') }}` |

### How `ref()` Works

```sql
-- models/marts/fct_trips.sql
select *
from {{ ref('int_trips_unioned') }}  -- References the int_trips_unioned model
```

**What `ref()` does:**
1. âœ… Resolves to the correct schema/table name
2. âœ… Builds the dependency graph automatically
3. âœ… Ensures models run in the correct order

### The DAG (Directed Acyclic Graph)

dbt builds a **dependency graph** from your `ref()` calls:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ stg_green_trips  â”‚     â”‚ stg_yellow_trips â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ int_trips_unionedâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    fct_trips     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

When you run `dbt build`, models run in dependency order automatically!

---

### Seeds - Loading CSV Files ğŸŒ±

Seeds let you load small CSV files into your warehouse as tables.

### When to Use Seeds

âœ… **Good use cases:**
- Lookup tables (zone names, country codes)
- Static mappings (vendor ID â†’ vendor name)
- Small reference data that rarely changes

âŒ **Not good for:**
- Large datasets (use proper data loading)
- Frequently changing data

### How to Use Seeds

1. **Put CSV files in the `seeds/` folder:**

```
seeds/
â””â”€â”€ taxi_zone_lookup.csv
```

```csv
locationid,borough,zone,service_zone
1,EWR,Newark Airport,EWR
2,Queens,Jamaica Bay,Boro Zone
3,Bronx,Allerton/Pelham Gardens,Boro Zone
...
```

2. **Run `dbt seed`:**

```bash
dbt seed
```

3. **Reference in models using `ref()`:**

```sql
-- models/marts/dim_zones.sql
select
    locationid as location_id,
    borough,
    zone,
    service_zone
from {{ ref('taxi_zone_lookup') }}
```

---

## Part 3: Testing, Documentation & Deployment ğŸš€

### Macros - Reusable SQL Functions ğŸ”§

Macros are like functions in Python - write once, use everywhere.

### Why Use Macros?

Without macros, you repeat code:

```sql
-- âŒ Repeated everywhere
CASE 
    WHEN payment_type = 1 THEN 'Credit card'
    WHEN payment_type = 2 THEN 'Cash'
    WHEN payment_type = 3 THEN 'No charge'
    WHEN payment_type = 4 THEN 'Dispute'
    WHEN payment_type = 5 THEN 'Unknown'
    ELSE 'Unknown'
END as payment_type_description
```

With macros, write it once:

```sql
-- macros/get_payment_type_description.sql
{% macro get_payment_type_description(payment_type) %}
    CASE {{ payment_type }}
        WHEN 1 THEN 'Credit card'
        WHEN 2 THEN 'Cash'
        WHEN 3 THEN 'No charge'
        WHEN 4 THEN 'Dispute'
        WHEN 5 THEN 'Unknown'
        ELSE 'Unknown'
    END
{% endmacro %}
```

Use it in any model:

```sql
-- models/staging/stg_green_tripdata.sql
select
    payment_type,
    {{ get_payment_type_description('payment_type') }} as payment_type_description
from {{ source('staging', 'green_tripdata') }}
```

### Jinja Templating

dbt uses **Jinja** - a Python templating language. You'll recognize it by `{{ }}` and `{% %}`:

| Syntax | Purpose | Example |
|--------|---------|---------|
| `{{ }}` | Output expression | `{{ ref('my_model') }}` |
| `{% %}` | Logic/control flow | `{% if is_incremental() %}` |
| `{# #}` | Comments | `{# This is a comment #}` |

---

### dbt Packages - Community Libraries ğŸ“¦

Packages let you use macros and models built by others.

### Popular Packages

| Package | What it Does |
|---------|--------------|
| **dbt_utils** | Common SQL helpers (surrogate keys, pivot, etc.) |
| **dbt_codegen** | Auto-generate YAML and SQL |
| **dbt_expectations** | Great Expectations-style tests |
| **dbt_audit_helper** | Compare model outputs when refactoring |

### Installing Packages

1. **Create `packages.yml`:**

```yaml
packages:
  - package: dbt-labs/dbt_utils
    version: 1.1.1
```

2. **Run `dbt deps`:**

```bash
dbt deps
```

3. **Use the macros:**

```sql
-- Using dbt_utils to generate surrogate keys
select
    {{ dbt_utils.generate_surrogate_key(['vendorid', 'pickup_datetime']) }} as trip_id,
    *
from {{ source('staging', 'green_tripdata') }}
```

---

### Testing in dbt ğŸ§ª

Tests ensure your data meets expectations. dbt has several test types:

**1. Generic Tests (Most Common)**

Built-in tests you apply in YAML:

```yaml
# models/staging/schema.yml
version: 2

models:
  - name: stg_green_tripdata
    columns:
      - name: trip_id
        tests:
          - unique       # No duplicate values
          - not_null     # No null values
      
      - name: payment_type
        tests:
          - accepted_values:
              values: [1, 2, 3, 4, 5, 6]  # Only these values allowed
      
      - name: pickup_location_id
        tests:
          - relationships:  # Referential integrity
              to: ref('dim_zones')
              field: location_id
```

**The four built-in tests:**
| Test | What it Checks |
|------|----------------|
| `unique` | No duplicate values in column |
| `not_null` | No NULL values in column |
| `accepted_values` | Values must be in specified list |
| `relationships` | Values must exist in another table |

**2. Singular Tests**

Custom SQL tests in the `tests/` folder:

```sql
-- tests/assert_positive_fare_amount.sql
-- Test FAILS if any rows are returned

select
    trip_id,
    fare_amount
from {{ ref('fct_trips') }}
where fare_amount < 0  -- Find negative fares (bad data!)
```

**3. Source Freshness Tests**

Check if your source data is up to date:

```yaml
sources:
  - name: staging
    tables:
      - name: green_tripdata
        freshness:
          warn_after: {count: 24, period: hour}
          error_after: {count: 48, period: hour}
        loaded_at_field: pickup_datetime
```

### Running Tests

```bash
# Run all tests
dbt test

# Run tests for specific model
dbt test --select stg_green_tripdata

# Run tests and models together
dbt build
```

---

### Documentation ğŸ“

dbt generates beautiful documentation automatically!

### Adding Descriptions

In your schema YAML:

```yaml
version: 2

models:
  - name: fct_trips
    description: >
      Fact table containing all taxi trips (yellow and green).
      One row per trip with fare details and zone information.
    
    columns:
      - name: trip_id
        description: Unique identifier for each trip (surrogate key)
      
      - name: service_type
        description: Type of taxi service - 'Yellow' or 'Green'
      
      - name: total_amount
        description: Total trip cost including fare, tips, taxes, and fees
```

### Generating Docs

```bash
# Generate documentation
dbt docs generate

# Serve locally (opens browser)
dbt docs serve
```

This creates an interactive website with:
- Model descriptions
- Column definitions
- Dependency graph (visual DAG)
- Source information

---

### Essential dbt Commands ğŸ’»

### The Big Four

| Command | What it Does |
|---------|--------------|
| `dbt run` | Build all models (create views/tables) |
| `dbt test` | Run all tests |
| `dbt build` | Run + test together (recommended!) |
| `dbt compile` | Generate SQL without executing |

### Other Useful Commands

```bash
# Check connection
dbt debug

# Load seed files
dbt seed

# Install packages
dbt deps

# Generate docs
dbt docs generate

# Retry failed models
dbt retry
```

### Selecting Specific Models

Use `--select` (or `-s`) to run specific models:

```bash
# Single model
dbt run --select stg_green_tripdata

# Model and all upstream dependencies
dbt run --select +fct_trips

# Model and all downstream models
dbt run --select stg_green_tripdata+

# Both directions
dbt run --select +fct_trips+

# All models in a folder
dbt run --select staging.*

# Multiple models
dbt run --select stg_green_tripdata stg_yellow_tripdata
```

### Target Environments

```bash
# Development (default)
dbt run

# Production
dbt run --target prod
```

---

### Materializations - Views vs Tables ğŸ“Š

Materialization controls how dbt persists your models in the warehouse.

### Types of Materializations

| Type | What it Creates | Use Case |
|------|-----------------|----------|
| **view** | SQL view (query stored, runs on access) | Staging models, frequently changing logic |
| **table** | Physical table (data stored) | Final marts, large datasets, performance |
| **incremental** | Appends new data only | Very large tables, event data |
| **ephemeral** | Not created (CTE in downstream) | Helper models, intermediate steps |

### Setting Materializations

**In the model file:**
```sql
{{ config(materialized='table') }}

select * from {{ ref('stg_trips') }}
```

**In dbt_project.yml (project-wide):**
```yaml
models:
  my_project:
    staging:
      materialized: view
    marts:
      materialized: table
```

### View vs Table Decision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Should I use view or table?                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Is the query expensive?  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚            â”‚
                    Yes          No
                     â”‚            â”‚
                     â–¼            â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  TABLE  â”‚  â”‚  VIEW   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Use VIEW when:**
- Staging models (simple transformations)
- Logic changes frequently
- Storage cost is a concern

**Use TABLE when:**
- Final marts queried often
- Complex joins/aggregations
- Query performance matters

---

### Putting It All Together - The NYC Taxi Project ğŸš•

In this module, we build a complete dbt project for NYC taxi data:

### What We Build

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAW DATA                                 â”‚
â”‚  green_tripdata (GCS/BigQuery) â”‚ yellow_tripdata (GCS/BigQuery)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                     â”‚
                    â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STAGING LAYER                              â”‚
â”‚      stg_green_tripdata    â”‚    stg_yellow_tripdata          â”‚
â”‚      (cleaned, renamed)    â”‚    (cleaned, renamed)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  INTERMEDIATE LAYER                           â”‚
â”‚                   int_trips_unioned                           â”‚
â”‚            (green + yellow combined)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MARTS LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ dim_zones   â”‚  â”‚   fct_trips   â”‚  â”‚fct_monthly_zone_rev â”‚ â”‚
â”‚  â”‚ (dimension) â”‚  â”‚    (fact)     â”‚  â”‚     (report)        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Models We Create

| Model | Type | Description |
|-------|------|-------------|
| `stg_green_tripdata` | Staging | Cleaned green taxi data |
| `stg_yellow_tripdata` | Staging | Cleaned yellow taxi data |
| `int_trips_unioned` | Intermediate | Combined yellow + green trips |
| `dim_zones` | Dimension | Zone lookup table |
| `fct_trips` | Fact | One row per trip |
| `fct_monthly_zone_revenue` | Report | Monthly revenue by zone |

---

### Setup Options ğŸ”§

### Option 1: Local Setup (DuckDB + dbt Core)

**Pros:** Free, no cloud account needed
**Cons:** Limited to your machine's power

```bash
# 1. Install dbt with DuckDB adapter
pip install dbt-duckdb

# 2. Clone the project
git clone https://github.com/DataTalksClub/data-engineering-zoomcamp
cd data-engineering-zoomcamp/04-analytics-engineering/taxi_rides_ny

# 3. Create profiles.yml in ~/.dbt/
# 4. Run dbt debug to test connection
dbt debug

# 5. Build the project
dbt build --target prod
```

### Option 2: Cloud Setup (BigQuery + dbt Cloud)

**Pros:** Powerful, team collaboration, scheduler
**Cons:** Requires GCP account (free tier available)

1. Create dbt Cloud account (free)
2. Connect to your BigQuery project
3. Clone the repo in dbt Cloud IDE
4. Run `dbt build --target prod`

---

### Troubleshooting Common Issues ğŸ”

### "Profile not found"
- Check `dbt_project.yml` profile name matches `profiles.yml`
- Ensure `profiles.yml` is in `~/.dbt/`

### "Source not found"
- Verify database/schema names in `sources.yml`
- Check your data is actually loaded in the warehouse

### "Model depends on model that was not found"
- Check for typos in `ref()` calls
- Ensure referenced model exists

### DuckDB Out of Memory
- Add memory settings to profiles.yml:
```yaml
settings:
  memory_limit: '2GB'
```

---

### Key Takeaways ğŸ“

1. **Analytics Engineering** bridges data engineering and data analysis

2. **dbt** brings software engineering best practices to SQL transformations

3. **Dimensional modeling** organizes data into facts (events) and dimensions (attributes)

4. **Three layers** - staging (raw copy), intermediate (transformations), marts (final)

5. **`ref()` and `source()`** are your main functions for building dependencies

6. **Testing** ensures data quality - use unique, not_null, accepted_values, relationships

7. **Documentation** is auto-generated from YAML descriptions

8. **`dbt build`** runs and tests everything in dependency order

---

### Additional Resources ğŸ“š

- [dbt Documentation](https://docs.getdbt.com/)
- [dbt Fundamentals Course](https://learn.getdbt.com/courses/dbt-fundamentals) (free)
- [SQL Refresher for Window Functions](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/04-analytics-engineering/refreshers/SQL.md)
- [dbt Community Slack](https://community.getdbt.com/)

---

## Submission

Homework form: https://courses.datatalks.club/de-zoomcamp-2026/homework/hw4
